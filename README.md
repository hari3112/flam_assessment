# flam_assessment

<img width="795" height="762" alt="image" src="https://github.com/user-attachments/assets/95f827dd-6efc-4b29-b2b5-ab72a2ef9ad6" />

Regression refers to finding the relationship between inputs and outputs by fitting a model to observed data.
In this case, the model is a known parametric function with unknown parameters θ,M,X.
The goal is to estimate these parameters so that the curve generated by the function fits the observed (x,y) data points as closely as possible.
This involves minimizing the error (e.g., L1 distance) between predicted and actual points, which is the core of a regression problem.

It is nonlinear because the model involves exponentials and trigonometric functions with parameters inside.
It differs from typical machine learning regression where the model structure is learned from data; here, the model form is known, and parameters are tuned.
So, it is nonlinear parametric regression or curve fitting using optimization techniques.
In short, the problem is best characterized as nonlinear regression by parameter estimation.

Parameter Estimation: We are finding specific parameters θ and X that best fit the known parametric equations to the given data. The function form is fixed, and we optimize parameters to minimize error.

# Summary
We are solving a mathematical optimization problem to estimate parameters of a known function.
There is no model learning or generalization step beyond fitting this specific dataset.
This approach is common in curve fitting, system identification, and classical regression rather than ML model training.
So to sum up: The process is about estimating the correct parameter values so the model fits the input data well, not about training a machine learning model.

To solve for the unknown parameters θ, M and X in the given parametric equations from the data points, the problem can be approached as a parameter estimation or curve fitting task using optimization.
The parameter t should be sampled uniformly between 6 and 60 at the same points (or approximate) used to generate data. Let's implement the parametric equations as functions of t,θ,M,X. Convert θ from degrees to radians since trigonometric functions use radians.The objective is to minimize the L1 distance (sum of absolute differences) between the predicted points and the observed points. L1 distance, also known as Manhattan Distance or Taxicab Distance, is a way to measure the difference between two points (or vectors) by summing the absolute differences of their corresponding components.:

<img width="602" height="70" alt="image" src="https://github.com/user-attachments/assets/9d9dc1d8-3c4c-4b4e-9101-36ead7f3db43" />

Instead of measuring the shortest "as-the-crow-flies" distance like Euclidean distance, L1 distance measures movement restricted to grid-like paths (like city blocks in Manhattan).It sums how far apart the points are in each dimension without squaring or taking square roots.L1 distance is used as the loss metric to measure how closely the parametric curve fits the observed data points.It penalizes deviations linearly, making it robust to outliers compared to squared error losses.

# How L1 regularization affects model coefficients

L1 regularization (also known as Lasso regularization) affects model coefficients by adding a penalty equal to the absolute sum of the coefficients to the loss function during model training. This penalty encourages the model to reduce some coefficient values to exactly zero.
Key Effects on Model Coefficients:
Sparsity: L1 regularization promotes sparsity in the coefficients because it can shrink irrelevant or less important feature weights to zero, effectively removing those features from the model.
Feature Selection: By driving some coefficients to zero, it performs implicit feature selection, simplifying the model and improving interpretability.
Bias-Variance Tradeoff: Increasing the regularization strength makes more coefficients zero, which reduces model complexity and variance but could increase bias (underfitting if too strong).
Grouping Effect: For correlated features, L1 tends to select one representative and zero out the others, unlike L2 which shrinks coefficients smoothly but keeps them non-zero.


It is sensitive to individual coordinate differences and favors sparser differences (useful in some machine learning and optimization contexts).
Let's use a nonlinear optimization method to find θ,M,X that minimize the loss. 
The curve parameters θ,M,X influence how the parametric equations generate points.

By feeding known t values and trying different parameters, the model produces estimated (x,y) points.
The optimizer searches parameter space to minimize the sum of absolute differences (L1 distance) between actual and estimated points.
Choosing L-BFGS-B method allows constrained optimization respecting the parameter bounds.
After optimization, the parameters that best fit the data according to L1 loss are reported.

# L-BFGS-B Algorithm

Here are the optimized parameter values and minimized L1 loss:

Optimized θ (degrees): approximately 28.1184 deg
Optimized M: 0.021389025229938657
Optimized X: approximately 54.902077030396846
Total L1 distance: 37865.09386302573

Visualization or comparison of fitted curve points against original data points using L-BFGS-B algorithm

<img width="1092" height="685" alt="image" src="https://github.com/user-attachments/assets/4f0ead17-f597-4f27-9164-959f81657c8e" />

# Plot in the desmos

<img width="1476" height="520" alt="image" src="https://github.com/user-attachments/assets/8b40e4cf-c93d-49b8-922a-5513c6a824ac" />

<img width="1853" height="866" alt="image" src="https://github.com/user-attachments/assets/9af4b353-5059-482c-95c0-e6eda4ac949b" />


But let's also have a look at the other algorithms execution and how it affects the L1 distance.



# Conjugate Gradient (CG)

Theta (degrees): 28.055194216546376
M: 0.021130574025268224
X: 54.468885853563066
Total L1 distance: 37868.46644682922

<img width="1118" height="683" alt="image" src="https://github.com/user-attachments/assets/86e31e99-f969-474e-bd98-625946231844" />

# COBYLA

Theta (degrees): 26.219343135398617
M: 0.01661150150455633
X: 52.02897474329058
Total L1 distance: 38096.69310981865

<img width="1810" height="793" alt="image" src="https://github.com/user-attachments/assets/507fe4f8-57b5-4c40-aaa6-2de5df80e886" />


<img width="1103" height="683" alt="image" src="https://github.com/user-attachments/assets/9d4a486b-39c5-4612-84f8-90104737ab71" />


| Algorithm              | Theta (degrees) | M      | X       | Total L1 Distance | Typical Use                            | Global Search |
| ---------------------- | --------------- | ------ | ------- | ----------------- | -------------------------------------- | ------------- |
| COBYLA                 | 26.2193         | 0.0166 | 52.0290 | 38096.6931        | Nonlinear constraints, derivative-free | No            |
| CG                     | 28.0552         | 0.0211 | 54.4689 | 37868.4664        | Large-scale smooth, gradient-based     | No            |
| L-BFGS-B               | 28.1184         | 0.0214 | 54.9021 | 37865.0939        | Bound constraints, efficient gradient  | No            |
| TNC                    | 28.2019         | 0.0219 | 54.8985 | 37865.2279        | Bound constrained optimization         | No            |
| Nelder-Mead            | 28.1184         | 0.0214 | 54.8992 | 37865.0938        | Unconstrained, derivative-free         | No            |
| Powell                 | 28.1180         | 0.0214 | 54.9113 | 37865.1163        | Bound constrained, derivative-free     | No            |
| Trust-Region Constr.   | 28.1184         | 0.0214 | 54.8996 | 37865.0938        | Nonlinear constraints, least squares   | No            |
| SLSQP                  | 28.1185         | 0.0214 | 54.8995 | 37865.0939        | Bound and nonlinear constraints        | No            |
| Basin-Hopping          | 28.1184         | 0.0214 | 54.9007 | 37865.0938        | Global stochastic-local descent hybrid | Yes           |
| Simulated Annealing    | 28.1188         | 0.0214 | 54.9003 | 37865.0944        | Global stochastic optimization         | Yes           |
| Differential Evolution | 28.1185         | 0.0214 | 54.8990 | 37865.0940        | Global evolutionary algorithm          | Yes           |




